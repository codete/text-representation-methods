{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Basic methods of text encoding\n",
    "\n",
    "There is plenty of simple methods which allow to perform text vectorization. They allow to encode the meaning of given textual data, but not surprisingly miss most of the contextual information. This notebook is a short recap of these basic ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words (BOG)\n",
    "\n",
    "Bag of Words is probably the simplest vectorization method one can imagine. During the training phase, it retrieves all the words used in provided corpus, forms a dictionary of all the used ones (so finally each word has a separate position in the output vector) and due to that allows to vectorize given text by counting the occurences of each word from it.\n",
    "\n",
    "Let's consider a simple example, written with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have a language corpus. Let's use some quotes of Milan Kundera (source: https://www.goodreads.com/author/quotes/6343.Milan_Kundera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = (\n",
    "    \"Two people in love, alone, isolated from the world, that's beautiful.\",\n",
    "    \"You can't measure the mutual affection of two human beings by the number of words they exchange.\",\n",
    "    \"Anyone whose goal is 'something higher' must expect someday to suffer vertigo. What is vertigo? \"\n",
    "    \"Fear of falling? No, Vertigo is something other than fear of falling. It is the voice of the emptiness \"\n",
    "    \"below us which tempts and lures us, it is the desire to fall, against which, terrified, we defend ourselves.\",\n",
    "    \"When the heart speaks, the mind finds it indecent to object.\",\n",
    "    \"Dogs are our link to paradise. They don't know evil or jealousy or discontent. To sit with a dog on a hillside \"\n",
    "    \"on a glorious afternoon is to be back in Eden, where doing nothing was not boring - it was peace.\",\n",
    "    \"Making love with a woman and sleeping with a woman are two separate passions, not merely different but opposite. \"\n",
    "    \"Love does not make itself felt in the desire for copulation (a desire that extends to an infinite number of women) \"\n",
    "    \"but in the desire for shared sleep (a desire limited to one woman).\",\n",
    "    \"Love is the longing for the half of ourselves we have lost.\",\n",
    "    \"for there is nothing heavier than compassion. Not even one's own pain weighs so heavy as the pain one feels with \"\n",
    "    \"someone, for someone, a pain intensified by the imagination and prolonged by a hundred echoes.\",\n",
    "    \"But when the strong were too weak to hurt the weak, the weak had to be strong enough to leave.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example for vectorization, it's probably better to have a sentence out of the corpora. We'll consider another quote of Kundera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE = \"A person who longs to leave the place where he lives is an unhappy person.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the human perspective, there is no difference if word is written with capital letters or not. For the vectorization that's a huge difference - if we just took the data without any preprocessing, we would simply see different positions assigned to words \"Anyone\" and \"anyone\". There are also some other issue like how to merge different words into the same position (\"speaks\" and \"speak\" hold the same information, but would be also encoded like if they were different). We'll consider it later on. First of all, let's try to fit our vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['affection', 'afternoon', 'against', 'alone', 'an', 'and', 'anyone', 'are', 'as', 'back', 'be', 'beautiful', 'beings', 'below', 'boring', 'but', 'by', 'can', 'compassion', 'copulation', 'defend', 'desire', 'different', 'discontent', 'does', 'dog', 'dogs', 'doing', 'don', 'echoes', 'eden', 'emptiness', 'enough', 'even', 'evil', 'exchange', 'expect', 'extends', 'fall', 'falling', 'fear', 'feels', 'felt', 'finds', 'for', 'from', 'glorious', 'goal', 'had', 'half', 'have', 'heart', 'heavier', 'heavy', 'higher', 'hillside', 'human', 'hundred', 'hurt', 'imagination', 'in', 'indecent', 'infinite', 'intensified', 'is', 'isolated', 'it', 'itself', 'jealousy', 'know', 'leave', 'limited', 'link', 'longing', 'lost', 'love', 'lures', 'make', 'making', 'measure', 'merely', 'mind', 'must', 'mutual', 'no', 'not', 'nothing', 'number', 'object', 'of', 'on', 'one', 'opposite', 'or', 'other', 'our', 'ourselves', 'own', 'pain', 'paradise', 'passions', 'peace', 'people', 'prolonged', 'separate', 'shared', 'sit', 'sleep', 'sleeping', 'so', 'someday', 'someone', 'something', 'speaks', 'strong', 'suffer', 'tempts', 'terrified', 'than', 'that', 'the', 'there', 'they', 'to', 'too', 'two', 'us', 'vertigo', 'voice', 'was', 'we', 'weak', 'weighs', 'were', 'what', 'when', 'where', 'which', 'whose', 'with', 'woman', 'women', 'words', 'world', 'you']\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(CORPUS)\n",
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already trained the vectorizer, we can simply use it to check how it encodes a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform((SENTENCE, )).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('an', 1), ('is', 1), ('leave', 1), ('the', 1), ('to', 1), ('where', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_scores(vectorizer, text):\n",
    "    X = vectorizer.transform((text, )).todense()\n",
    "    mapping = zip(vectorizer.get_feature_names(), X.tolist()[0])\n",
    "    return list(filter(lambda pair: pair[1] > 0.0, mapping))\n",
    "    \n",
    "get_word_scores(count_vectorizer, SENTENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually lost all the information from the sentence used for encoding. What's more, if we just consider any permutation of the original sentence, the encoded vector will be exactly the same. Bag of words method doesn't care about the context of words, but just encoded their presence in given text. It may be used for some simple cases, but typically is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "\n",
    "When we use BOG, there is another drawback - some non-informative words have exactly the same weight like the ones carrying the essential pieces of information. So called stopwords may be removed at the very beginning, but there are also some other words which may be common for our corpus, but not for the whole language. That's where TFIDF comes to the rescue!\n",
    "\n",
    "TFIDF stands for Term Frequency Inversed Document Frequency. In simple words - for each word we calculate two different terms and multiply them:\n",
    "- **TF** - term frequency, is a popularity score for a particular word and document - the higher the value, the more common the word is in a provided text document\n",
    "- **IDF** - inversed term frequency, is a inversed popularity score for a particular word, in all the corpus documents - the higher the value, the less common the word is in a whole corpus, so the more informative it is (if a word is very common, then typically it doesn't affect the meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to consider the same corpus and the same sentence like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['affection', 'afternoon', 'against', 'alone', 'an', 'and', 'anyone', 'are', 'as', 'back', 'be', 'beautiful', 'beings', 'below', 'boring', 'but', 'by', 'can', 'compassion', 'copulation', 'defend', 'desire', 'different', 'discontent', 'does', 'dog', 'dogs', 'doing', 'don', 'echoes', 'eden', 'emptiness', 'enough', 'even', 'evil', 'exchange', 'expect', 'extends', 'fall', 'falling', 'fear', 'feels', 'felt', 'finds', 'for', 'from', 'glorious', 'goal', 'had', 'half', 'have', 'heart', 'heavier', 'heavy', 'higher', 'hillside', 'human', 'hundred', 'hurt', 'imagination', 'in', 'indecent', 'infinite', 'intensified', 'is', 'isolated', 'it', 'itself', 'jealousy', 'know', 'leave', 'limited', 'link', 'longing', 'lost', 'love', 'lures', 'make', 'making', 'measure', 'merely', 'mind', 'must', 'mutual', 'no', 'not', 'nothing', 'number', 'object', 'of', 'on', 'one', 'opposite', 'or', 'other', 'our', 'ourselves', 'own', 'pain', 'paradise', 'passions', 'peace', 'people', 'prolonged', 'separate', 'shared', 'sit', 'sleep', 'sleeping', 'so', 'someday', 'someone', 'something', 'speaks', 'strong', 'suffer', 'tempts', 'terrified', 'than', 'that', 'the', 'there', 'they', 'to', 'too', 'two', 'us', 'vertigo', 'voice', 'was', 'we', 'weak', 'weighs', 'were', 'what', 'when', 'where', 'which', 'whose', 'with', 'woman', 'women', 'words', 'world', 'you']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit_transform(CORPUS)\n",
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('an', 0.504069491273459),\n",
       " ('is', 0.32706807618956785),\n",
       " ('leave', 0.504069491273459),\n",
       " ('the', 0.2135243418310226),\n",
       " ('to', 0.2918487157505274),\n",
       " ('where', 0.504069491273459)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_scores(tfidf_vectorizer, SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
